#---Remove unnecessary columns---
df = df[['summary', 'genre']]
df.info()

#---Remove samples with 2 labels---
duplicates = df.groupby("summary")["genre"].nunique()
texts_to_remove = duplicates[duplicates > 1].index.tolist()
df = df[~df["summary"].isin(texts_to_remove)]

# ---Handle Missing Data---
# Check missing data
missing_count = df.isnull().sum()
total_missing = missing_count.sum()

if total_missing == 0:
    print('\033[1mMissing Data:\033[0m No missing values')
else:
    missing_pct = (missing_count / len(df) * 100).round(2)
    print('\033[1mMissing Data (%):\033[0m')
    print(missing_pct[missing_percent > 0])

# Handle missing data (if any)

# ---Handle Duplicate Data---
# Check duplicate data
dup_count = df.duplicated().sum()
total_rows = len(df)

if dup_count == 0:
    print('\033[1mDuplicate Data:\033[0m No duplicate rows')
else:
    dup_pct = (dup_count/ total_rows * 100).round(2)
    print('\033[1mDuplicate Data:\033[0m')
    print(f'Duplicate rows: {dup_count}')
    print(f'% Duplicate: {dup_pct}%')

#Handle duplicate data (if any)
df = df.drop_duplicates()
print(df.info())

# ---Remove Special char---
import re

# Check special char
all_text = ' '.join(df['summary'].astype(str))
total_chars = len(all_text)
spc_chars = re.findall(r'[^a-zA-Z0-9\s]', all_text)
spc_char_count = len(spc_chars)

if spc_char_count == 0:
    print('\033[1mSpecial Chars:\033[0m No special chars')
else:
    spc_char_pct = round(spc_char_count/ total_chars * 100, 2)

    print(f"Total chars: {total_chars}")
    print(f"Total specical chars: {spc_char_count}")
    print(f"% special chars: {spc_char_pct}%")

# Handle special char
def remove_spc_chars(text):

    text = str(text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.lower()
    return text

df['summary'] = df['summary'].apply(remove_spc_chars)

#Check
#total_chars_new = len(' '.join(df['summary_clean'].astype(str)))
#print(total_chars_new)

#---Remove Stop Words and Outliers---
df['word_count'] = df['summary'].str.split().str.len()
Q1 = df['word_count'].quantile(0.25)
Q3 = df['word_count'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

df = df[(df['word_count'] >= lower) & (df['word_count'] <= upper)].reset_index(drop=True)
print("After IQR filtering shape:", df.shape)

#---Split train-test dataset---
df_ml = df[['summary', 'genre']]
X_text = df_ml['summary']
y_raw = df_ml['genre']

X_train, X_test, y_train, y_test = train_test_split(
    X_text, y_raw, test_size=0.2, random_state=42, stratify=y_raw
)

print('Done split train-test')

#---TF-IDF---

from sklearn.feature_extraction.text import TfidfVectorizer

# Tạo TF-IDF vectorizer, loại bỏ stop words tiếng Anh
tfidf = TfidfVectorizer(
    max_features=10000,      # số lượng từ đặc trưng tối đa
    ngram_range=(1,3),      # unigram + bigram
    stop_words='english'    # tự động loại stop words
)

# fit on train dataset
X_train_tfidf = tfidf.fit_transform(X_train)

# Transform on test dataset
X_test_tfidf = tfidf.transform(X_test)

#---Encoding lable---
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)
class_names = le.classes_
